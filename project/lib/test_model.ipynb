{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model\n",
    "This notebook establishes layers for the CNN (evaluating image size, memory requirements).\n",
    "a) It experiments with the Keras \"Model\" and \"Layers\" using examples from the documentaion.\n",
    "b) It looks at examples from the book (Geras) and internet on approaches to structuring a CNN model\n",
    "c) It then goes into a deep dive looking at AlexNet.\n",
    "\n",
    "Keras References:\n",
    "https://keras.io/models/model/\n",
    "https://keras.io/layers/about-keras-layers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 3)                 15        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 2)                 6         \n",
      "=================================================================\n",
      "Total params: 21\n",
      "Trainable params: 21\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sequential Model\n",
    "# https://keras.io/layers/core/#dense\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# without bias...\n",
    "# the first (input) stage is 5 inputs * 3 nodes = 15\n",
    "# the second stage is 3 inputs * 2 nodes = 6\n",
    "model = Sequential()\n",
    "model.add(Dense(units=3, input_shape=(5,), use_bias=False))\n",
    "model.add(Dense(units=2, use_bias=False))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 3)                 18        \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 2)                 8         \n",
      "=================================================================\n",
      "Total params: 26\n",
      "Trainable params: 26\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# with bias...\n",
    "# Bias is applied at the output... (?)\n",
    "\n",
    "# the first (input) stage is 5 inputs * 3 nodes = 15\n",
    "# the second stage is 3 inputs * 2 nodes = 6\n",
    "model = Sequential()\n",
    "model.add(Dense(units=3, input_shape=(5,), use_bias=True))\n",
    "model.add(Dense(units=2, use_bias=True))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 2.3407 - acc: 0.1020\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 321us/step - loss: 2.3143 - acc: 0.1080\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 272us/step - loss: 2.2994 - acc: 0.1230\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 302us/step - loss: 2.2904 - acc: 0.1350\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 292us/step - loss: 2.2820 - acc: 0.1560\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 299us/step - loss: 2.2740 - acc: 0.1450\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 286us/step - loss: 2.2652 - acc: 0.1550\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 289us/step - loss: 2.2592 - acc: 0.1570\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 310us/step - loss: 2.2499 - acc: 0.1750\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 297us/step - loss: 2.2412 - acc: 0.1690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0efb58b470>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# categorical output\n",
    "# https://keras.io/getting-started/sequential-model-guide/\n",
    "\n",
    "# For a single-input model with 10 classes (categorical classification):\n",
    "\n",
    "# The questions are... why:\n",
    "# \"relu\"\n",
    "# \"softmax\"\n",
    "# conversion to one-hot encoding\n",
    "# \"cross-entropy\"\n",
    "\n",
    "# And the answers are:\n",
    "# To what is \"cross-entropy\" applied?   Answer:  cross-entropy is applied\n",
    "# during training as the distance measure between input X and output y.\n",
    "# The gradient-descent will use this to determine how to adjust all the\n",
    "# weights in the network.\n",
    "#\n",
    "# Conversion to one-hot encoding?  \n",
    "# Answer: the author converts Y into a one-hot encoded value before using.\n",
    "# The reason behind this - we want Y to be categorical - and the easiest\n",
    "# way to generate random categorical data is to first generate\n",
    "# a random integer and then convert that to one-hot-binary representation.\n",
    "# I think this is missing some important aspects - that categorical representation\n",
    "# implies a probabilistic intent.  If so then the label data should really not be one-hot-binary\n",
    "# encoded but rather one-hot-probability encoded.  That is for the labels as\n",
    "# given to \"fit\" for training - note during runtime (\"predict\") the\n",
    "# network will output floats (and the output function is \"softmax\" which guarantees\n",
    "# it will be between 0 and 1).  But there is no assurance they SUM to 1 which is what\n",
    "# we would expect for a Random Variable.   So if your goal is just to classify you can use the\n",
    "# output and do a maxarg() on it.  But if your goal is to output a random variable then\n",
    "# it may be necessary to normalize the output.\n",
    "#\n",
    "# Experiment 1: Confirm the network as is outputs values between 0 and 1, but they do not sum to 1.\n",
    "# Experiment 2: There is no reason we could not input labels which ae one-hot-probability (would\n",
    "# be good experiment)\n",
    "\n",
    "import keras\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(10, size=(1000, 1))\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, one_hot_labels, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9999999\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# - OK so it is confirmed that the network output sums to 1.0\n",
    "# predict(x, batch_size=None, verbose=0, steps=None)\n",
    "\n",
    "out = model.predict(data[0:4])\n",
    "\n",
    "print(out[0].sum())\n",
    "print(out[1].sum())\n",
    "print(out[2].sum())\n",
    "print(out[3].sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Trying to architect the layers for my CNN...\n",
    "\n",
    "Image size is 480x640.\n",
    "The site where the car is running will determine the color of the surface and\n",
    "color of the guideline. For example - Gene's track is a red tape on a carpet (dark on light)\n",
    "while mine is white tape on a concrete floor (light on dark).  The point\n",
    "is we cannot use color as an indicator, or even that the line is light\n",
    "relative to the surface.  Rather - \n",
    "the concept of an \"edge\" (any light-to-dark transition) will be used.  This will\n",
    "allow the car to run at either site.\n",
    "Ambient lighting will vary - the overall image may be dark or light.\n",
    "The line width varies due to perspective and due to width of the material (tape vs rope).\n",
    "At its furthest distance the tape is maybe 10 pixels, at its nearest point it is 100 pixels.\n",
    "The line may extend into the distance, at its longst it is 3/4 the height of the image.    \n",
    "\n",
    "As such:\n",
    "\n",
    "Early Processing Stages - here we are removing common mode signals\n",
    "    Remove color (convert to Black/White)\n",
    "    Normalize\n",
    "    Augmentation -> invert the image (swap black and white)\n",
    "    (At this point we could try a threshold detect (convert to binary) but we will leave\n",
    "    that as a future exercise.)\n",
    "\n",
    "First stage of processing - here we are performing localized Feature Detection (detecting\n",
    "edge of the line) and then a pair of edges which constitue a 'line crossing'\n",
    "    Single edge detection - the highest frequency of our 'signal' is 10 pixels. Nyquist says\n",
    "    we need to sample at least twice that frequency. But neural net nodes look at a set of pixels\n",
    "    (multiple samples)...  At the end of the day we want the edge detect filter to be wide enough\n",
    "    to detect the fuzziest edge transition.  Additional width is OK - those pixels will be\n",
    "    'trained out' i.e. and the additional computation is nominal.\n",
    "    \n",
    "    The plan is to start small\n",
    "    Width=6  Stride=3.\n",
    "    We can later experiment with larger width/strides.\n",
    "\n",
    "    We should be able to 'pool' - get rid of the original image.\n",
    "    \n",
    "Mid-Stages of processing\n",
    "    Here we are up-sampling by convolution.  First to get an 'edge-pair',\n",
    "    and then combine to get continuity in the long dimension and the overall\n",
    "    position in the image.\n",
    "\n",
    "We then go to fully-connected layers.\n",
    "\n",
    "The final (outout) layer will need to be categorical - our data (\"y\") is\n",
    "    one-of-three signals (left,center,right).\n",
    "\n",
    "=+=========\n",
    "\n",
    "The images from the camera are mid-sized (480x640).\n",
    "The examples such as the \"Le-Net architecture (Geron - Hands-On Machine Learning - page 370)\n",
    "use teeny images - 32x32.\n",
    "Will I be able to train/run it (ballpark) ?\n",
    "\n",
    "a) Memory Use (during training)\n",
    "Refererring to:\n",
    "https://ai.stackexchange.com/questions/3938/how-to-handle-images-of-large-sizes-in-cnn\n",
    "480 * 640 * 3 channels * 4 bytes/ch * batch size of 10 == 37MB\n",
    "So the memory requirement is 37MB for images during training.\n",
    "The parameters and model itself are probably not significant.\n",
    "Summary - we think we will be OK for memory during training.\n",
    "\n",
    "Compute cycles during training - this depends on batch size/epoch and number of\n",
    "paramters, and how fast it reaches a minimim (threshold).  Hard to say.\n",
    "\n",
    "b) If the feature is localized then selective search and pick region of interest.\n",
    "https://www.researchgate.net/post/How_to_modify_a_Convolutional_Neural_Network_architecture_to_deal_with_large_input_images\n",
    "This is not my case but a good approach.\n",
    "\n",
    "c) CUDA to offload training to a GPU. This can also applied at runtime if you have a local GPU.\n",
    "d) during training - split the load across cloud compute nodes - i.e. tensorflow !\n",
    "\n",
    "======================================\n",
    "THE QUESTION IS - how do I get from 480x630 down to \"3\"\n",
    "It appears from the LeNet example - the pooling layers divide\n",
    "the number of features in half - because stride = 2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1 - AlexNet\n",
    "Here for my edification I am re-documenting the \"AlexNet\" (Geron page 371).\n",
    "Unfortunately I don't have the compile, fit or predict steps.  Since I \n",
    "expect the runtime would be excessive (i.e. would need to be run on cloud\n",
    "server with multiple GPUs) I'm not going to pursue it beyond 'add' and 'summary.\n",
    "Even with that it is instructive.\n",
    "\n",
    "Notes and code to implement AlexNet follow.\n",
    "\n",
    "Notes:\n",
    "This application of CNN focuses on image processing.  As such the\n",
    "data is expressed in 2-d.  The information extracted is therefore kept\n",
    "in the third dimension (depth) which is the number of maps.\n",
    "And we observe as layers progress the \"image\" size (x,y dimensions) decrease, while the third\n",
    "dimension increases.  That is - as \"information\" is extracted from the \"data\"\n",
    "there is more  contained in the third dimension and less in the first two.\n",
    "\n",
    "<Aside - would be interesting to develop an 'encoding' of where something is located\n",
    "in 3-d space.  The encoding would use random variables (i.e. be expressed as\n",
    "the combination of categorical likelihoods) and would utilize\n",
    "reference points in 3-d space to describe the 'item of focus'.  The question might\n",
    "be how to you express the presence of more than one item?  Or does this\n",
    "model support only the item of focus ?  (given an item of interes, where is that\n",
    "item in the 3-d space?>\n",
    "\n",
    "We believe there are three \"strata\" here: information extraction, application, and rgb extraction\n",
    "Image extraction is the midle strata - extracting all information possible from the\n",
    "2-d image, but preserving the \"location\" of that data as the 2-d representation\n",
    "Application is the third and final stage in the strata.  It is specific to the\n",
    "application - creating a one-hot detector.\n",
    "RGB extraction is the first (input) strata - this is eliminating the RGB content\n",
    "to vacate the the third dimension so it can be used for information extraction.\n",
    "\n",
    "```\n",
    "\n",
    "Name    Type          #maps stride/pad kernel activation resulting dimensions    \n",
    "\n",
    "in                                                     3(RGB) 224 x 224\n",
    "C1   Convolution        96  4 \"SAME\"    11x11 ReLU\n",
    "                                                           96  55 x 55\n",
    "S2   Max Pooling            2 \"VALID\"    3x3\n",
    "                                                           96  27 x 27\n",
    "C3   Convolution       256  1 \"SAME\"     5x5  ReLU\n",
    "                                                          256  27 x 27\n",
    "S4   Max Pooling            2 \"VALID\"   13x13\n",
    "                                                          256  13 x 13\n",
    "C5   Convolution       384  1 \"SAME\"     3x3  ReLU\n",
    "                                                          384  13 x 13\n",
    "C6   Convolution       384  1 \"SAME\"     3x3  ReLU\n",
    "                                                          384  13 x 13\n",
    "C7   Convolution       256  1 \"SAME\"     3x3  ReLU\n",
    "                                                          256  13 x 13\n",
    "F8   Fully Connected  4096                    ReLU\n",
    "                                                         4096 x 1\n",
    "F9   Fully Connected  4096                    ReLU\n",
    "                                                         4096 x 1\n",
    "Out  Fully Connected  1000                    Softmax\n",
    "                                                         1000 x 1\n",
    "```\n",
    "The code follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 56, 56, 96)        34944     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 27, 27, 256)       614656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 8, 8, 384)         885120    \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 8, 8, 384)         1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 8, 8, 256)         884992    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8, 8, 4096)        1052672   \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 8, 8, 4096)        16781312  \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 8, 8, 1000)        4097000   \n",
      "=================================================================\n",
      "Total params: 25,678,184\n",
      "Trainable params: 25,678,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Here is the code for AlexNet:\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D\n",
    "\n",
    "#    #params = (11x11x3*96 = 34,48 = 34848) + bias (11x11x3 = 363) = 35,211\n",
    "c1 = Conv2D(filters=96, strides=(4, 4), padding='same', kernel_size=(11,11), activation='relu', input_shape=(224, 224, 3),  use_bias=True) \n",
    "s2 = MaxPooling2D(strides=2,  padding='valid', pool_size=(3, 3))\n",
    "c3 = Conv2D(filters=256, strides=(1, 1), padding='same', kernel_size=(5,5), activation='relu')\n",
    "s4 = MaxPooling2D(strides=2, padding='valid', pool_size=(13, 13))\n",
    "c5 = Conv2D(filters=384, strides=(1, 1), padding='same', kernel_size=(3,3), activation='relu')\n",
    "c6 = Conv2D(filters=384, strides=(1, 1), padding='same', kernel_size=(3,3), activation='relu')\n",
    "c7 = Conv2D(filters=256, strides=(1, 1), padding='same', kernel_size=(3,3), activation='relu')\n",
    "f8 = Dense(units=4096, activation='relu')\n",
    "f9 = Dense(units=4096, activation='relu')\n",
    "fout = Dense(units=1000, activation='softmax')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(c1)\n",
    "model.add(s2)\n",
    "model.add(c3)\n",
    "model.add(s4)\n",
    "model.add(c5)\n",
    "model.add(c6)\n",
    "model.add(c7)\n",
    "model.add(f8)\n",
    "model.add(f9)\n",
    "model.add(fout)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2 - \"Simple Image Classification\" from web...\n",
    "```\n",
    "The next example is from https://becominghuman.ai/building-an-image-classifier-using-deep-learning-in-python-totally-from-a-beginners-perspective-be8dbaf22dd8\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-526478bc487b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mdisplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_image_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"control100\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filename' is not defined"
     ]
    }
   ],
   "source": [
    "from metrowestcar_file_io import FileReader\n",
    "from metrowestcar_display import Displayer\n",
    "\n",
    "from os import getcwd\n",
    "from os.path import abspath\n",
    "from os.path import join\n",
    "from os.path import exists\n",
    "\n",
    "\n",
    "file_reader = FileReader()\n",
    "displayer = Displayer()\n",
    "\n",
    "fullpath = join(abspath(getcwd()), \"../data/pictures_test\")\n",
    "image_array = file_reader.read_images_from_directory(fullpath)\n",
    "for i in image_array:\n",
    "    displayer.display_image(i)\n",
    "\n",
    "image = file_reader.read_image_from_file(join(fullpath,filename))\n",
    "\n",
    "filename = \"control100\"\n",
    "steering = file_reader.read_steering_from_file(join(fullpath,filename))\n",
    "\n",
    "# displaying an image\n",
    "\n",
    "\n",
    "\n",
    "#displayer.display_thumbnail(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
